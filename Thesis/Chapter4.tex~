\section{Linear models}
  In statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X. In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models.
Linear Regression fits a linear model with coefficients $w = (w_1, ..., w_p)$ to minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation. Mathematically it solves a problem of the form:
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.2\linewidth]{linearreg}                   
\end{figure}
\newline
However, coefficient estimates for Ordinary Least Squares rely on the independence of the model terms. When terms are correlated and the columns of the design matrix X have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed response, producing a large variance. 
\subsection{Ridge Regression}
Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares,
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\linewidth]{ridgereg}
    \label{fig:ridgereg}                    
\end{figure}
\newline
Here, $\alpha \geq 0$ is a complexity parameter that controls the amount of shrinkage: the larger the value of $\alpha$, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.
\newpage
\section{Non-Linear models}
  When the data has a non-linear relationship between dependent and independent variables, then non-linear models tend to perform better than linear models.
  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{non_linear}
    \caption{Intutively, non-linear model fits better.}
    \label{fig:non_linear}                    
\end{figure}
  \subsection{Artificial Neural Networks}
  Artificial Neural Networks are a class of Machine Learning Algorithms inspired from the neurons of the human body. This is biologically inspired machine learning algorithm. A neuron is a single unit that has several inputs with associated weights and an activation function. The weights are used to represent the strength of each input. Depending on the amount of activation, the neuron produces its own activity and sends this along its outputs.
  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{neuron}
    \caption{Neurons in human brain}
    \label{fig:neuron}                    
  \end{figure}
  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{ann}
    \caption{Artificial neuron}
    \label{fig:ann}                    
  \end{figure}
  \newpage
  
The artificial equivalent of a neuron is a node that receives a set of weighted inputs, processes their sum with its activation function \( \phi \) \, and passes the result of the activation function to nodes further down the graph. We can then form a network by chaining these nodes together. Usually this is done in layers that is one layer's outputs are connected to the next layer's inputs.
Our problem is a supervised classification problem and hence we feed the training labeled training set to the network. Training in this case involves learning the correct edge weights to produce the target output given the input. The network then generalizes for new inputs.
The evaluation method is binary cross entropy. Convex optimization is used to minimize the objective function and uses Stochastic Gradient Descent.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{annlayer}
    \caption{Artificial neuron with various layers}
    \label{fig:annlayer}                    
\end{figure}

The ANN is trained using Back-Propagation algorithm.
This is a method for updating the weights by propagating the error backwards, starting from the output layer, until each neuron has an associated error value which roughly represents its contribution to the original output. 
\newline
Here the Loss function is SSE (sum-squared error).
\begin{equation}
  f(x) = \sigma (Wx + b)
\end{equation}
We have used the sigmoid activation function which is
\begin{equation}
  \sigma (z) = e^z/(e^z+1)
\end{equation} 
